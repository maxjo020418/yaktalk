"""
ë²•ë ¹ ì •ë³´ API í˜¸ì¶œ ë° ChromaDB ì €ì¥ ëª¨ë“ˆ
"""

import os
import sys
import requests
import re
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€ (ì§ì ‘ ì‹¤í–‰ ì‹œ)
sys.path.append(str(Path(__file__).parent.parent))

from langchain_core.tools import tool
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

from utils.custom_embeddings import get_law_embeddings
from utils.get_env import OLLAMA_SERVER_URL, OPEN_LAW_GO_ID


@dataclass
class LawConfig:
    """ë²•ë ¹ API ì„¤ì •"""
    api_base_url: str = "http://www.law.go.kr/DRF/lawService.do"
    search_base_url: str = "http://www.law.go.kr/DRF/lawSearch.do"
    chunk_size: int = 1024
    chunk_overlap: int = 100
    max_articles: int = 50
    search_threshold: int = 2
    max_distance_score: float = 350.0  # ìµœëŒ€ ê±°ë¦¬ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬) - Ollama embeddings scale
    timeout: int = 10


@dataclass
class ArticleInfo:
    """ì¡°ë¬¸ ì •ë³´"""
    jo: str = ''
    hang: str = ''
    ho: str = ''
    
    def format_reference(self, law_name: str) -> str:
        """ì¡°í•­ ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·"""
        reference = law_name
        if self.jo:
            reference += f" ì œ{self.jo}ì¡°"
        if self.hang:
            reference += f" ì œ{self.hang}í•­"
        if self.ho:
            reference += f" ì œ{self.ho}í˜¸"
        return reference


class LawAPIClient:
    """ë²•ë ¹ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: LawConfig | None = None):
        self.config = config or LawConfig()
        self.api_key = OPEN_LAW_GO_ID
    
    def search_laws(self, query: str) -> Optional[Dict]:
        """ë²•ë ¹ ê²€ìƒ‰"""
        params = {
            'OC': self.api_key,
            'target': 'law',
            'type': 'JSON',
            'query': query,
            'display': '10'
        }
        
        return self._make_request(self.config.search_base_url, params)
    
    def get_law_by_id(self, law_id: str) -> Optional[Dict]:
        """ë²•ë ¹ IDë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.api_key,
            'target': 'law',
            'type': 'JSON',
            'ID': law_id
        }
        
        return self._make_request(self.config.api_base_url, params)
    
    def get_law_by_mst(self, mst: str) -> Optional[Dict]:
        """ë²•ë ¹ ì¼ë ¨ë²ˆí˜¸ë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.api_key,
            'target': 'law',
            'type': 'JSON',
            'MST': mst
        }
        
        return self._make_request(self.config.api_base_url, params)
    
    def _make_request(self, url: str, params: Dict) -> Optional[Dict]:
        """API ìš”ì²­ ì‹¤í–‰"""
        try:
            response = requests.get(url, params=params, timeout=self.config.timeout)
            if response.status_code == 200:
                print(f"request success: {response.request.url}")
                return response.json()
        except Exception as e:
            print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None


class LawDocumentProcessor:
    """ë²•ë ¹ ë¬¸ì„œ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: LawConfig | None = None):
        self.config = config or LawConfig()
    
    def parse_article_number(self, article_num: str) -> ArticleInfo:
        """ì¡°ë¬¸ë²ˆí˜¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì¡°/í•­/í˜¸ ì •ë³´ ì¶”ì¶œ"""
        if not article_num:
            return ArticleInfo()
        
        jo_match = re.search(r'ì œ(\d+)ì¡°', article_num)
        hang_match = re.search(r'ì œ(\d+)í•­', article_num)
        ho_match = re.search(r'ì œ(\d+)í˜¸', article_num)
        
        return ArticleInfo(
            jo=jo_match.group(1) if jo_match else '',
            hang=hang_match.group(1) if hang_match else '',
            ho=ho_match.group(1) if ho_match else ''
        )
    
    def create_law_documents(self, law_data: Dict) -> List[Document]:
        """ë²•ë ¹ ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜"""
        documents = []
        
        if 'ë²•ë ¹' not in law_data:
            return documents
            
        law_info = law_data['ë²•ë ¹']
        
        # ê¸°ë³¸ ì •ë³´ ë¬¸ì„œ ìƒì„±
        basic_info = self._create_basic_info(law_info)
        documents.append(basic_info)
        
        # ì¡°ë¬¸ ì •ë³´ ë¬¸ì„œ ìƒì„±
        article_docs = self._create_article_documents(law_info)
        documents.extend(article_docs)
        
        return documents
    
    def _create_basic_info(self, law_info: Dict) -> Document:
        """ë²•ë ¹ ê¸°ë³¸ ì •ë³´ ë¬¸ì„œ ìƒì„±"""
        # ê¸°ë³¸ì •ë³´ê°€ ì¤‘ì²©ë˜ì–´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        basic_info = law_info.get('ê¸°ë³¸ì •ë³´', law_info)
        
        # ì†Œê´€ë¶€ì²˜ ì •ë³´ ì²˜ë¦¬ - ì¤‘ì²©ëœ ê°ì²´ì¸ ê²½ìš°ì™€ ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
        department = basic_info.get('ì†Œê´€ë¶€ì²˜')
        if isinstance(department, dict):
            department_name = department.get('content', 'N/A')
        else:
            department_name = department or basic_info.get('ì†Œê´€ë¶€ì²˜ëª…', 'N/A')
        
        content = f"""
        ë²•ë ¹ëª…: {basic_info.get('ë²•ë ¹ëª…_í•œê¸€', 'N/A')}
        ë²•ë ¹ID: {basic_info.get('ë²•ë ¹ID', 'N/A')}
        ê³µí¬ì¼ì: {basic_info.get('ê³µí¬ì¼ì', 'N/A')}
        ì‹œí–‰ì¼ì: {basic_info.get('ì‹œí–‰ì¼ì', 'N/A')}
        ì†Œê´€ë¶€ì²˜: {department_name}
        """
        
        return Document(
            page_content=content.strip(),
            metadata={
                'type': 'law_basic',
                'law_id': basic_info.get('ë²•ë ¹ID', ''),
                'law_name': basic_info.get('ë²•ë ¹ëª…_í•œê¸€', '')
            }
        )
    
    def _create_article_documents(self, law_info: Dict) -> List[Document]:
        """ì¡°ë¬¸ ì •ë³´ ë¬¸ì„œë“¤ ìƒì„±"""
        documents = []
        
        # ì¡°ë¬¸ êµ¬ì¡° ì²˜ë¦¬: ì¡°ë¬¸.ì¡°ë¬¸ë‹¨ìœ„ ë˜ëŠ” ì§ì ‘ ì¡°ë¬¸
        articles_container = law_info.get('ì¡°ë¬¸', {})
        if isinstance(articles_container, dict) and 'ì¡°ë¬¸ë‹¨ìœ„' in articles_container:
            articles = articles_container['ì¡°ë¬¸ë‹¨ìœ„']
        else:
            articles = law_info.get('ì¡°ë¬¸', [])
            
        if isinstance(articles, dict):
            articles = [articles]
        
        # ê¸°ë³¸ì •ë³´ì—ì„œ ë²•ë ¹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        basic_info = law_info.get('ê¸°ë³¸ì •ë³´', law_info)
        
        for article in articles[:self.config.max_articles]:
            if isinstance(article, dict):
                doc = self._create_single_article_document(article, basic_info)
                if doc:
                    documents.append(doc)
        
        return documents
    
    def _create_single_article_document(self, article: Dict, basic_info: Dict) -> Optional[Document]:
        """ë‹¨ì¼ ì¡°ë¬¸ ë¬¸ì„œ ìƒì„±"""
        article_num = article.get('ì¡°ë¬¸ë²ˆí˜¸', '')
        article_title = article.get('ì¡°ë¬¸ì œëª©', '')
        article_content = article.get('ì¡°ë¬¸ë‚´ìš©', '')
        
        if not article_content:
            return None
        
        article_info = self.parse_article_number(article_num)
        
        content = f"""
        ì¡°ë¬¸ë²ˆí˜¸: {article_num}
        ì¡°ë¬¸ì œëª©: {article_title}
        ì¡°ë¬¸ë‚´ìš©: {article_content}
        """.strip()
        
        return Document(
            page_content=content,
            metadata={
                'type': 'law_article',
                'law_id': basic_info.get('ë²•ë ¹ID', ''),
                'law_name': basic_info.get('ë²•ë ¹ëª…_í•œê¸€', ''),
                'article_title': article_title,
                'article_number': article_num,
                'jo': article_info.jo,
                'hang': article_info.hang,
                'ho': article_info.ho
            }
        )


class LawVectorStore:
    """ë²•ë ¹ ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬"""
    
    def __init__(self, config: LawConfig | None = None):
        self.config = config or LawConfig()
        self.processor = LawDocumentProcessor(config)
        self.vectorstore = None
        self._initialize()
    
    def _initialize(self):
        """ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            embeddings = get_law_embeddings(OLLAMA_SERVER_URL, "nomic-embed-text")
            
            persist_directory = Path(__file__).parent.parent / "database" / "law_chroma_db"
            persist_directory.mkdir(parents=True, exist_ok=True)
            
            # langchain-chromaì˜ ìƒˆë¡œìš´ API ì‚¬ìš©
            self.vectorstore = Chroma(
                collection_name="law_documents",
                embedding_function=embeddings,
                persist_directory=str(persist_directory)
            )

            assert self.vectorstore is not None, f"Vectorstore must be initialized"
            print(f"âœ… ë²•ë ¹ ChromaDB ì´ˆê¸°í™” ì™„ë£Œ: {persist_directory}")
        except Exception as e:
            print(f"âš ï¸ ë²•ë ¹ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("í™˜ê²½ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”. (OLLAMA_SERVER_URL, ëª¨ë¸ ê°€ìš©ì„± ë“±)")
            self.vectorstore = None
    
    def add_law_data(self, law_data: Dict) -> int:
        """ë²•ë ¹ ë°ì´í„°ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€"""
        if not self.vectorstore:
            print("âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0
            
        documents = self.processor.create_law_documents(law_data)
        
        if not documents:
            return 0
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        splits = text_splitter.split_documents(documents)
        
        if splits:
            self.vectorstore.add_documents(splits)
            print(f"âœ… {len(splits)}ê°œì˜ ë²•ë ¹ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")
        
        return len(splits)
    
    def search(self, query: str, k: int = 5) -> List[Document]:
        """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰"""
        if not self.vectorstore:
            print("âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
        retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )
        return retriever.invoke(query) or []
    
    def search_with_scores(self, query: str, k: int = 5) -> List[tuple[Document, float]]:
        """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰"""
        if not self.vectorstore:
            print("âŒ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # Ollama embeddingsì—ì„œëŠ” similarity_search_with_scoreê°€ ë” ì•ˆì •ì 
        return self.vectorstore.similarity_search_with_score(query, k=k) or []
    
    def is_sufficient_result(self, docs_with_scores: List[tuple[Document, float]], query: str) -> bool:
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨ (ì ìˆ˜ ê¸°ë°˜)"""
        if not docs_with_scores or len(docs_with_scores) < self.config.search_threshold:
            print(f"ğŸ” ê²°ê³¼ ë¶€ì¡±: {len(docs_with_scores) if docs_with_scores else 0}ê°œ (ìµœì†Œ {self.config.search_threshold}ê°œ í•„ìš”)")
            return False
        
        # ê±°ë¦¬ ì ìˆ˜ê°€ ì¶©ë¶„íˆ ë‚®ì€(ìœ ì‚¬í•œ) ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
        # ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ë” ìœ ì‚¬í•¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ìœ ì‚¬)
        high_quality_results = [
            (doc, score) for doc, score in docs_with_scores 
            if score <= self.config.max_distance_score  # ê±°ë¦¬ ì ìˆ˜ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìŒ(ë” ìœ ì‚¬)
        ]
        
        print(f"ğŸ” ì ìˆ˜ í‰ê°€: {len(high_quality_results)}ê°œ ìƒìœ„ ê²°ê³¼ (ê±°ë¦¬ â‰¤{self.config.max_distance_score})")
        if docs_with_scores:
            scores = [score for _, score in docs_with_scores[:3]]
            print(f"ğŸ” ìƒìœ„ 3ê°œ ì ìˆ˜: {scores}")
        
        return len(high_quality_results) >= self.config.search_threshold


class LawService:
    """ë²•ë ¹ ì„œë¹„ìŠ¤ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = LawConfig()
        self.api_client = LawAPIClient(self.config)
        self.vector_store = LawVectorStore(self.config)
        self.processor = LawDocumentProcessor(self.config)
        self.cache: Dict[str, Any] = {}
    
    def search_laws(self, query: str) -> str:
        """ë²•ë ¹ ê²€ìƒ‰ ë©”ì¸ ë¡œì§"""
        print(f"ğŸ” ë²•ë ¹ ê²€ìƒ‰: '{query}'")
        
        # 1ë‹¨ê³„: ChromaDBì—ì„œ ê¸°ì¡´ ë²•ë ¹ ê²€ìƒ‰ (ì ìˆ˜ì™€ í•¨ê»˜)
        existing_docs_with_scores = self.vector_store.search_with_scores(query)
        
        # 2ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
        if self.vector_store.is_sufficient_result(existing_docs_with_scores, query):
            print("âœ… ChromaDBì—ì„œ ì¶©ë¶„í•œ ë²•ë ¹ ì •ë³´ ë°œê²¬")
            # ê±°ë¦¬ ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ (ë‚®ì€ ê±°ë¦¬ê°€ ë” ìœ ì‚¬í•¨)
            existing_docs_with_scores.sort(key=lambda x: x[1])  # ê±°ë¦¬ ë‚®ì€ ìˆœ (ë” ìœ ì‚¬í•œ ìˆœ)
            existing_docs = [doc for doc, score in existing_docs_with_scores[:5]]
            return self._format_results(existing_docs, "ê·¼ê±°ë²•ë ¹")
        
        # 3ë‹¨ê³„: API í˜¸ì¶œë¡œ ìƒˆë¡œìš´ ë²•ë ¹ ê°€ì ¸ì˜¤ê¸°
        print("âš¡ ChromaDBì— ê´€ë ¨ ë²•ë ¹ì´ ë¶€ì¡±í•˜ì—¬ API í˜¸ì¶œ ì¤‘...")
        
        law_data = self._fetch_law_data_by_query(query)
        
        if law_data:
            print("âœ… ìƒˆë¡œìš´ ë²•ë ¹ ë°ì´í„° íšë“, ChromaDBì— ì €ì¥ ì¤‘...")
            
            # ìºì‹œ ì €ì¥
            cache_key = query[:50]
            self.cache[cache_key] = law_data
            
            # ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
            self.vector_store.add_law_data(law_data)
            
            # ì—…ë°ì´íŠ¸ëœ DBì—ì„œ ë‹¤ì‹œ ê²€ìƒ‰
            updated_docs = self.vector_store.search(query)
            if updated_docs:
                return self._format_results(updated_docs, "ìƒˆë¡œ ì¶”ê°€ëœ ê·¼ê±°ë²•ë ¹")
        
        # ìµœì¢… ë‹¨ê³„: ê¸°ì¡´ ê²°ê³¼ë¼ë„ ë°˜í™˜
        if existing_docs_with_scores:
            print("âš ï¸ ìƒˆ ë²•ë ¹ ì¶”ê°€ ì‹¤íŒ¨, ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜")
            existing_docs = [doc for doc, score in existing_docs_with_scores[:3]]
            return self._format_results(existing_docs, "ê¸°ì¡´ ê·¼ê±°ë²•ë ¹")
        
        return "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def load_law_by_id(self, law_id: str | None = None, mst: str | None = None) -> str:
        """íŠ¹ì • ë²•ë ¹ IDë¡œ ë²•ë ¹ ì •ë³´ ë¡œë“œ"""
        law_data = None
        
        if law_id:
            law_data = self.api_client.get_law_by_id(law_id)
        elif mst:
            law_data = self.api_client.get_law_by_mst(mst)
        
        if law_data and 'ë²•ë ¹' in law_data:
            law_info = law_data['ë²•ë ¹']
            # ê¸°ë³¸ì •ë³´ì—ì„œ ì •ë³´ ì¶”ì¶œ
            basic_info = law_info.get('ê¸°ë³¸ì •ë³´', law_info)
            
            # ì†Œê´€ë¶€ì²˜ ì •ë³´ ì²˜ë¦¬
            department = basic_info.get('ì†Œê´€ë¶€ì²˜')
            if isinstance(department, dict):
                department_name = department.get('content', 'N/A')
            else:
                department_name = department or basic_info.get('ì†Œê´€ë¶€ì²˜ëª…', 'N/A')
                
            # ì¡°ë¬¸ ìˆ˜ ê³„ì‚°
            articles_container = law_info.get('ì¡°ë¬¸', {})
            if isinstance(articles_container, dict) and 'ì¡°ë¬¸ë‹¨ìœ„' in articles_container:
                articles = articles_container['ì¡°ë¬¸ë‹¨ìœ„']
            else:
                articles = law_info.get('ì¡°ë¬¸', [])
            
            if isinstance(articles, dict):
                articles = [articles]
                
            article_count = len(articles) if isinstance(articles, list) else 0
            
            self.vector_store.add_law_data(law_data)

            return f"ë²•ë ¹ëª…: {basic_info.get('ë²•ë ¹ëª…_í•œê¸€', 'N/A')}" \
                   f"\në²•ë ¹ID: {basic_info.get('ë²•ë ¹ID', 'N/A')}" \
                   f"\nê³µí¬ì¼ì: {basic_info.get('ê³µí¬ì¼ì', 'N/A')}" \
                   f"\nì‹œí–‰ì¼ì: {basic_info.get('ì‹œí–‰ì¼ì', 'N/A')}" \
                   f"\nì†Œê´€ë¶€ì²˜: {department_name}" \
                   f"\nì¡°ë¬¸ ìˆ˜: {article_count}ê°œ" \
                   f"\në²•ë ¹ ì •ë³´ê°€ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.".strip()
        
        return "ë²•ë ¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _fetch_law_data_by_query(self, query: str) -> Optional[Dict]:
        """ì¿¼ë¦¬ë¡œ ë²•ë ¹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        # ë¨¼ì € ê²€ìƒ‰ìœ¼ë¡œ ë²•ë ¹ ì°¾ê¸°
        search_result = self.api_client.search_laws(query) or {}
        search_result = search_result.get('LawSearch', [])

        if search_result and 'law' in search_result:
            laws = search_result.get('law', [])
            if laws:
                first_law = laws[0]  # only the first one
                law_id = first_law.get('ë²•ë ¹ID')
                if law_id:
                    # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    return self.api_client.get_law_by_id(law_id)

        print('âš ï¸ ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. [_fetch_law_data_by_query]')
        return None
    
    def _format_results(self, docs: List[Document], prefix: str, show_scores: bool = False, docs_with_scores: Optional[List[tuple[Document, float]]] = None) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…"""
        results = []
        
        # ì ìˆ˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if docs_with_scores:
            score_map = {id(doc): score for doc, score in docs_with_scores}
        else:
            score_map = {}
        
        for i, doc in enumerate(docs[:5], 1):
            # ì¡°í•­ ì •ë³´ í¬ë§·íŒ…
            article_info = ArticleInfo(
                jo=doc.metadata.get('jo', ''),
                hang=doc.metadata.get('hang', ''),
                ho=doc.metadata.get('ho', '')
            )
            
            law_name = doc.metadata.get('law_name', 'Unknown')
            article_ref = article_info.format_reference(law_name)
            content = doc.page_content[:500]
            
            # ì ìˆ˜ í‘œì‹œ (ë””ë²„ê¹…ìš©)
            score_info = ""
            if show_scores and id(doc) in score_map:
                score = score_map[id(doc)]
                score_info = f" (ì ìˆ˜: {score:.3f})"
            
            results.append(f"[{prefix} {i}]{score_info} {article_ref}\n{content}")
        
        return "\n\n".join(results)


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_law_service = LawService()


@tool
def search_law_by_query(query: str) -> str:
    """
    Search online for legal information and return relevant content.
    Use only one or two keywords to query.
    
    Returns:
        Relevant legal information
    """
    return _law_service.search_laws(query.split()[0])


@tool
def load_law_by_id(law_id: str | None = None, mst: str | None = None) -> str:
    """
    Load legal information by specific law ID or serial number.
    
    Returns:
        Legal information
    """
    return _law_service.load_law_by_id(law_id, mst)


# Export tools
tools = [search_law_by_query, load_law_by_id]